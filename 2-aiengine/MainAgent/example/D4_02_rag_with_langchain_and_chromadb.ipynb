{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba06737c-2448-4d51-9da2-fdbdc9a58c03",
   "metadata": {},
   "source": [
    "### RAG Introduction with Langchain/Chromadb and LLAMACPP (OPENAI-API)\n",
    "\n",
    "Here we built a RAG Application using library provided functions. \n",
    "We we will use \n",
    "- [LLAMA.CPP](https://github.com/ggml-org/llama.cpp) as the engine\n",
    "  to run embedding and large language models.\n",
    "- [ChromaDB](https://www.trychroma.com/) as an open source vector\n",
    "  database\n",
    "- [LangChain](https://python.langchain.com) as prompt engineering, RAG\n",
    "  framework\n",
    "\n",
    "We implement a chatbot that shall correspond with a PDF file of the \n",
    "following article about the evolution of the omicron COVID-19 variant\n",
    "in France: [Retrospective analysis of SARS-CoV-2 omicron invasion over delta in French regions in 2021â€“22: a status-based multi-variant model](https://bmcinfectdis.biomedcentral.com/articles/10.1186/s12879-022-07821-5)\n",
    "\n",
    "As such we have downloaded the PDF version of the Article and converted it\n",
    "to plain text with `pdftotext` which comes with the [Poppler library](https://poppler.freedesktop.org/). The resulting file is provided as `s12879-022-07821-5.txt\n",
    "\n",
    "For this course we need to run models using the [LLAMA.CPP](https://github.com/ggml-org/llama.cpp) engine in the background.\n",
    "We shall serve the `all-MiniLM-L6-v2` embedding model:\n",
    "```\n",
    "llama-server -m all-MiniLM-L6-v2-Q8_0.gguf --host localhost --port 8081 --embedding\n",
    "```\n",
    "and the `Llama-3.2-3B-Instruct` LLM:\n",
    "```\n",
    "llama-server -m Llama-3.2-3B-Instruct-Q8_0.gguf -c 10000 --host localhost --port 8080\n",
    "``` \n",
    "with a maximum context size of 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec44552-c8c3-451d-b12c-9c0151b9af3c",
   "metadata": {},
   "source": [
    "Here we start with a general initialization of our system. As multiple participents of this course run their code on the same node we want to generate random port numbers to provide our REST services. Further we obtain the systems an IP address to bind our REST servers (embedding, llm) to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8863f1e7-23b9-4bff-b08f-a0c0be29bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import socket\n",
    "ip = socket.gethostbyname(socket.gethostname())\n",
    "embedding_port = str(random.randint(40000,50000))\n",
    "llm_port = str(random.randint(40000,50000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0eb98b-9899-4115-b42d-72a05adc004d",
   "metadata": {},
   "source": [
    "We launch the rest servers (llama-cpp) in background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f36de1f2-9716-48eb-8cb8-b9674c7819d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, DEVNULL\n",
    "\n",
    "public_root = '/leonardo/pub/userexternal/thaschka/'\n",
    "llama_cpp_server = public_root + 'llama.cpp/bin/llama-server'\n",
    "llm_model = public_root + 'llm/llama-3.1-8B-I-Q8.gguf'\n",
    "embedding_model = public_root + 'embed/all-MiniLM-L6-v2-Q8_0.gguf' \n",
    "\n",
    "embedding_process = Popen([llama_cpp_server, \n",
    "                           '-m', embedding_model, \n",
    "                           '--host', ip, \n",
    "                           '--port', embedding_port, \n",
    "                           '--embedding'],\n",
    "                          stdout=DEVNULL,\n",
    "                          stderr=DEVNULL)\n",
    "\n",
    "llm_process = Popen([llama_cpp_server,\n",
    "                     '-m', llm_model,\n",
    "                     '--host', ip,\n",
    "                     '--port', llm_port,\n",
    "                     '-c', '10000',\n",
    "                     '-ngl', '32'],\n",
    "                     stdout=DEVNULL,\n",
    "                     stderr=DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ac7d7-ac65-4000-bda5-dd1733d2a5c2",
   "metadata": {},
   "source": [
    "Just to assure that we have the right encoding on the machine we set everything to UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b63191a7-1a62-491a-be36-c0b6284ca7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9328c75a-92a8-4eb1-9238-bba9eac3e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "f = open(public_root + 's12879-022-07821-5.txt','r')\n",
    "text = f.read()\n",
    "wrapped_text = wrap(text,1000)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4c79c-e16b-44df-b91a-3e2b97d27cc4",
   "metadata": {},
   "source": [
    "We wrapped the text into chunks of containing a maximum of 1000 characters each. *wrap* stops at spaces, and hence the chunks are a bit smaller. We have obtained 53 such chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3f86b-7051-4e20-a715-28cb04d640e1",
   "metadata": {},
   "source": [
    "Langchain provides us with the REST api to talk to our local llama.cpp models. \n",
    "As llama.cpp uses the same API as OPENAI we can use the OPENAI model. \n",
    "There are however some quirks with the embedding API, which should be compatible, \n",
    "but is not, as such we use langchain's LocalAIEmbeddings API for the embeddings,\n",
    "which works together with our llama.cpp server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2fc388-b238-4e67-84f8-2bef7dfa600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_localai import LocalAIEmbeddings\n",
    "llm = ChatOpenAI(openai_api_base='http://' + ip + ':' + llm_port + '/v1',\n",
    "                openai_api_key=\"whatever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829e237-16c6-4585-af74-f508418658fe",
   "metadata": {},
   "source": [
    "Let us try thing out: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dc31b59-e264-4d07-ab23-fa6b924666d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm just a computer program, so I don't have feelings, but thank you for asking! I'm functioning properly and ready to help you with any questions or tasks you may have. How about you? How's your day going so far?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 16, 'total_tokens': 67, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'b5209-d2b2031e', 'id': 'chatcmpl-AgcQJ1CAm1yUXuocB8O2EQRGAjmKG4cs', 'finish_reason': 'stop', 'logprobs': None} id='run-fd856549-2beb-4f18-9bd1-859f8c3f9301-0' usage_metadata={'input_tokens': 16, 'output_tokens': 51, 'total_tokens': 67, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Hello how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceb985dc-dad4-4eea-a3fd-c8bed6b6c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = LocalAIEmbeddings(openai_api_base=\"http://\" + ip + \":\" +embedding_port + \"/v1\",\n",
    "                            openai_api_key=\"forget\",\n",
    "                            model=\"all-MiniLM-L6-v2\",\n",
    "                            openai_api_version='v1',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bedb379-d473-47b1-847c-a81fe21f2275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors.embed_documents(wrapped_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e5e3f-87d9-493a-8a86-5d3fc12a984c",
   "metadata": {},
   "source": [
    "In the next step we have to setup a vector database. \n",
    "Here as opposed to our first sample we use chromadb, but a large choice\n",
    "of databases would exist. Only execute this once or destroy your presist_directory\n",
    "and restart the notebooks' kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90e3370b-c785-4995-ad0e-fd3f1a954a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_texts(texts=wrapped_text, \n",
    "                             embedding=vectors, \n",
    "                             persist_directory=\"chroma_db\")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11d135-2357-4dec-99bc-a0cc9fc0add9",
   "metadata": {},
   "source": [
    "Just as in the last case we have to deal with a chat template.\n",
    "Here we can use langchains' tools in order include history and contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b63b9358-599a-46fd-b1c6-6c0a5487dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"<|start_header_id|>system<|end_header_id|> \\n\\n\" + \\\n",
    "    \"You are a helpful AI assistent in answering prompt, \" + \\\n",
    "    \"taking the following contexts into account \" + \\\n",
    "    \"as good as you can as you answer. \\n\\n\" + \\\n",
    "    \"Contexts:\\n\"\n",
    "    \"{context}<|eot_id|>\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\",\"<|start_header_id|>user<|end_header_id|> {input}<|eot_id|> \\n\\n\" + \\\n",
    "    \"<|start_header_id|>assistent<|end_header_id>\")])\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38f9da-2dc5-44cc-8ebd-67f310a66f73",
   "metadata": {},
   "source": [
    "Here we build our retrieval chain, with the template above. For each question we retrieve corresponding documents from the chromadb vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebbc0c70-6953-4d55-8cd6-398d71f778da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever,question_answer_chain) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a655e43-0ed4-481d-ba83-85951aed5b57",
   "metadata": {},
   "source": [
    "Finally we have also to handle the chat history and define a query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db94f403-83e9-4ec0-84c8-4d9cd2e7b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def chat_query(question):\n",
    "    a = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "    human_msg = \"<|start_header_id|>user<|end_header_id|>\" + question + \"<|eot_id|>\"\n",
    "    ai_answer = \"<|start_header_id|>assistent<|end_header_id|>\" + a['answer'] + \"<|eot_id|>\"\n",
    "\n",
    "    chat_history.extend([\n",
    "        (\"human\", human_msg),\n",
    "        (\"ai\", ai_answer)\n",
    "        ])\n",
    "    print(a['answer'])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fc636-6160-437f-82c1-a8fe7089f5fe",
   "metadata": {},
   "source": [
    "Now we can run our chatbot and ask some questions related to our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7e420b9-88a7-4334-8ceb-8578b44673ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20-day window of opportunity refers to a specific time period used in the study to analyze the replacement of the delta variant by the omicron variant in metropolitan France. This window is defined as a 20-day period that starts 10 days before the inflection point, where the omicron variant exceeds 50% of reported samples.\n",
      "\n",
      "The inflection point is considered to be the midpoint of the 20-day window, which is the 10th day. This means that the first 10 days of the window are used to model the initial growth of the omicron variant, and the last 10 days are used to model the replacement of the delta variant by the omicron variant.\n",
      "\n",
      "The authors of the study chose this 20-day window for several reasons:\n",
      "\n",
      "1. The omicron variant was still relatively rare 10 days prior to the inflection point, so the initial growth of the omicron variant could be accurately modeled.\n",
      "2. The 20-day window allows for a sufficient number of days to observe the replacement of the delta variant by the omicron variant.\n",
      "3. The window starts at a point where the omicron variant is present in more than half of the samples, which allows for a more accurate modeling of the replacement process.\n",
      "\n",
      "The 20-day window of opportunity is used to:\n",
      "\n",
      "1. Estimate the initial conditions of the model, including the proportions of delta and omicron variants.\n",
      "2. Fit the model to the data and estimate the parameters, such as the cross-immunity between the variants and the generation time of the omicron variant.\n",
      "3. Analyze the replacement of the delta variant by the omicron variant and estimate the transmission advantage of the omicron variant.\n",
      "\n",
      "The 20-day window of opportunity is a key aspect of the study, as it allows for a more accurate modeling of the replacement process and provides valuable insights into the dynamics of SARS-CoV-2 variant replacement.\n"
     ]
    }
   ],
   "source": [
    "a = chat_query(\"Tell me about the 20-day window of opportunity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "136f9c98-965c-408b-bc5f-993dfb156f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the study, the 20-day window of opportunity occurred in metropolitan France during the period from December 2021 to January 2022.\n",
      "\n",
      "The exact dates of the 20-day window of opportunity varied by region, but the study mentions that the invasion of the omicron variant, specifically the lineage BA.1, occurred approximately 3 weeks after its first detection in metropolitan France.\n",
      "\n",
      "Given that the omicron variant was first detected in France in early December 2021, the 20-day window of opportunity likely started around December 10-15, 2021, and ended around January 5-10, 2022.\n",
      "\n",
      "Here are the estimated dates for the 20-day window of opportunity in some of the French metropolitan regions, based on the study:\n",
      "\n",
      "* Ile-de-France region: December 12, 2021 - January 1, 2022\n",
      "* Hauts-de-France region: December 15, 2021 - January 4, 2022\n",
      "* Nouvelle-Aquitaine region: December 10, 2021 - January 30, 2022\n",
      "* Occitanie region: December 12, 2021 - January 2, 2022\n",
      "\n",
      "Please note that these dates are estimates and may vary slightly depending on the specific region and the timing of the omicron variant's invasion.\n"
     ]
    }
   ],
   "source": [
    "a = chat_query(\"At what dates did the 20-day window of opportunity happen in france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4a0e90-a0a0-4179-bd75-514538185b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef18bb87-f722-4bb7-8216-3bc8abf6cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d8b94-0cfb-4958-a5f9-514a9bb19183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
